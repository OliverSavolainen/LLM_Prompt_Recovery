{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":6229341,"sourceType":"datasetVersion","datasetId":3574875},{"sourceId":7844455,"sourceType":"datasetVersion","datasetId":4599344},{"sourceId":7893017,"sourceType":"datasetVersion","datasetId":4634330},{"sourceId":7925072,"sourceType":"datasetVersion","datasetId":4593930},{"sourceId":8004247,"sourceType":"datasetVersion","datasetId":4713888},{"sourceId":8056406,"sourceType":"datasetVersion","datasetId":4750212},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":170469403,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":785.677725,"end_time":"2024-04-06T20:34:28.334397","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-06T20:21:22.656672","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0d5615cd005541cbbf7dff5b2702d1e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e51ea986f514cf4bf6f06d6103a30a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0edca9296b9f44128e6127ff0178344f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f1352a6dfcf4d98b0cace7f43e8d875":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0edca9296b9f44128e6127ff0178344f","placeholder":"​","style":"IPY_MODEL_584ff6a77eca4bdba9f201798d695bc6","value":" 2/2 [00:03&lt;00:00,  1.84s/ba]"}},"2056fdcdc7f043c79482c571119e1051":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20fa8ddfd2cf4d0daa1ee788c48f2543":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9641672364043f7a6e05d8663cc450a","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab0ff06011c44065b4ac0722f7417825","value":2}},"266e21db5a3c45b3b4bb36ae185cd39a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d729a824d6c406a85e6719a18460dfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_266e21db5a3c45b3b4bb36ae185cd39a","placeholder":"​","style":"IPY_MODEL_eceeed213426427eb8bbfb25e107a131","value":"100%"}},"40de999f4f564dc99ce40c297a9329e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9bc7fa5fb39d41fca6447523a86a105a","IPY_MODEL_e6d412a0db6a4a80b00c004739d2c228","IPY_MODEL_9e907412fa7d46989056320da780ea31"],"layout":"IPY_MODEL_8a87fc7679a44ee4a504183b0993d45a"}},"4e9fa0147cf64f0c919072d52ea8b8d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"584ff6a77eca4bdba9f201798d695bc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84b4babc42c242fab3b98851bdc113ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a87fc7679a44ee4a504183b0993d45a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"929cbca3dbbb403eb4b3b297bece7bc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bc7fa5fb39d41fca6447523a86a105a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e9fa0147cf64f0c919072d52ea8b8d5","placeholder":"​","style":"IPY_MODEL_929cbca3dbbb403eb4b3b297bece7bc5","value":"Loading checkpoint shards: 100%"}},"9e907412fa7d46989056320da780ea31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84b4babc42c242fab3b98851bdc113ac","placeholder":"​","style":"IPY_MODEL_0d5615cd005541cbbf7dff5b2702d1e0","value":" 3/3 [01:42&lt;00:00, 33.76s/it]"}},"ab0ff06011c44065b4ac0722f7417825":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9641672364043f7a6e05d8663cc450a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc7ee21a036842a78203e7b60408e55e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2d729a824d6c406a85e6719a18460dfe","IPY_MODEL_20fa8ddfd2cf4d0daa1ee788c48f2543","IPY_MODEL_0f1352a6dfcf4d98b0cace7f43e8d875"],"layout":"IPY_MODEL_0e51ea986f514cf4bf6f06d6103a30a2"}},"e3c4c02c28fd41bd9fa2019654bfc311":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6d412a0db6a4a80b00c004739d2c228":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2056fdcdc7f043c79482c571119e1051","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3c4c02c28fd41bd9fa2019654bfc311","value":3}},"eceeed213426427eb8bbfb25e107a131":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Step 1 - Install necessary packages\nFirst, install the dependencies below to get started. As these features are available on the main branches only, we need to install the libraries below from source.","metadata":{"id":"juj9DSmmh-Gw","papermill":{"duration":0.046444,"end_time":"2024-04-06T20:21:25.804051","exception":false,"start_time":"2024-04-06T20:21:25.757607","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Click on \"File + Add Input\"\n# Search for the dataset named \"llm-prompt-recovery\"\n# Once you find it, click on \"Add\" to attach it to your Kernel\nADAPTER_MODEL_PATH = '/kaggle/input/d/valentinalilova/llm-prompt-recovery'\nTEST_CSV_PATH = ADAPTER_MODEL_PATH + '/test_example.csv'","metadata":{"execution":{"iopub.status.busy":"2024-04-07T20:10:32.415469Z","iopub.execute_input":"2024-04-07T20:10:32.415832Z","iopub.status.idle":"2024-04-07T20:10:32.428055Z","shell.execute_reply.started":"2024-04-07T20:10:32.415804Z","shell.execute_reply":"2024-04-07T20:10:32.427093Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/","metadata":{"papermill":{"duration":14.137683,"end_time":"2024-04-06T20:21:39.988542","exception":false,"start_time":"2024-04-06T20:21:25.850859","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-07T20:10:32.429507Z","iopub.execute_input":"2024-04-07T20:10:32.429910Z","iopub.status.idle":"2024-04-07T20:10:45.435246Z","shell.execute_reply.started":"2024-04-07T20:10:32.429875Z","shell.execute_reply":"2024-04-07T20:10:45.433952Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U accelerate --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U bitsandbytes --no-index --find-links ../input/transformers-4-38-2/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n!pip install /kaggle/input/peft-whl-latest/peft-0.10.0-py3-none-any.whl","metadata":{"id":"3nNWXXc7ol1n","outputId":"5dc39936-5be4-4a8d-d6c7-81026b9e9590","papermill":{"duration":75.735007,"end_time":"2024-04-06T20:22:55.771605","exception":false,"start_time":"2024-04-06T20:21:40.036598","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-07T20:10:45.437096Z","iopub.execute_input":"2024-04-07T20:10:45.437454Z","iopub.status.idle":"2024-04-07T20:11:57.375705Z","shell.execute_reply.started":"2024-04-07T20:10:45.437421Z","shell.execute_reply":"2024-04-07T20:11:57.374513Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Processing /kaggle/input/peft-whl-latest/peft-0.10.0-py3-none-any.whl\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (4.38.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (0.28.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (0.4.2)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (0.21.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.10.0) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.10.0) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.10.0) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.10.0) (1.3.0)\npeft is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U peft --no-index --find-links ../input/llm-pkg/","metadata":{"papermill":{"duration":13.018654,"end_time":"2024-04-06T20:23:08.839901","exception":false,"start_time":"2024-04-06T20:22:55.821247","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-07T20:11:57.377327Z","iopub.execute_input":"2024-04-07T20:11:57.377671Z","iopub.status.idle":"2024-04-07T20:12:10.329694Z","shell.execute_reply.started":"2024-04-07T20:11:57.377639Z","shell.execute_reply":"2024-04-07T20:12:10.328668Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T20:12:10.331192Z","iopub.execute_input":"2024-04-07T20:12:10.331522Z","iopub.status.idle":"2024-04-07T20:12:10.336859Z","shell.execute_reply.started":"2024-04-07T20:12:10.331492Z","shell.execute_reply":"2024-04-07T20:12:10.335852Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"## Step 2 - Model loading\nWe'll load the model using QLoRA quantization to reduce the usage of memory\n","metadata":{"id":"3NVvNhohkvwF","papermill":{"duration":0.048544,"end_time":"2024-04-06T20:23:08.936534","exception":false,"start_time":"2024-04-06T20:23:08.887990","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load base model(Mistral 7B)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n )\n\nmodel_id = \"/kaggle/input/mistral-7b-it-v02\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)","metadata":{"id":"kvvLg99Opw5R","outputId":"849c7439-3e52-47e2-cc34-7db52c3d594c","papermill":{"duration":112.379665,"end_time":"2024-04-06T20:25:01.364104","exception":false,"start_time":"2024-04-06T20:23:08.984439","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-07T20:12:10.339611Z","iopub.execute_input":"2024-04-07T20:12:10.339918Z","iopub.status.idle":"2024-04-07T20:12:22.088920Z","shell.execute_reply.started":"2024-04-07T20:12:10.339893Z","shell.execute_reply":"2024-04-07T20:12:22.088052Z"},"trusted":true},"execution_count":72,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9495d4024f864d0b9b36da80cfdffeaf"}},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftConfig, PeftModel\nmodel = PeftModel.from_pretrained(model, ADAPTER_MODEL_PATH)","metadata":{"id":"ghK_9dnz87GR","outputId":"18fc4f02-05d5-40c7-9fb2-76c72269901a","papermill":{"duration":7.054637,"end_time":"2024-04-06T20:25:08.467157","exception":false,"start_time":"2024-04-06T20:25:01.412520","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-07T20:12:22.090350Z","iopub.execute_input":"2024-04-07T20:12:22.091216Z","iopub.status.idle":"2024-04-07T20:12:22.660762Z","shell.execute_reply.started":"2024-04-07T20:12:22.091178Z","shell.execute_reply":"2024-04-07T20:12:22.659782Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T20:12:22.662280Z","iopub.execute_input":"2024-04-07T20:12:22.662553Z","iopub.status.idle":"2024-04-07T20:12:22.677270Z","shell.execute_reply.started":"2024-04-07T20:12:22.662530Z","shell.execute_reply":"2024-04-07T20:12:22.676294Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n              (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_prompt(original, rewritten):\n    return f\"Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\\nYou are trying to understand how the original essay was transformed into a new version.\\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\\nOutput should be only line ONLY\\nOriginal Essay: {original}\\nRewritten Essay: {rewritten}\"\n","metadata":{"id":"cDH2h3FNmNiD","outputId":"ecbbf8f7-1dfe-466d-a3ac-d2582dda7b41","papermill":{"duration":157.892668,"end_time":"2024-04-06T20:27:46.700609","exception":false,"start_time":"2024-04-06T20:25:08.807941","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-07T20:12:22.678580Z","iopub.execute_input":"2024-04-07T20:12:22.679009Z","iopub.status.idle":"2024-04-07T20:12:22.687371Z","shell.execute_reply.started":"2024-04-07T20:12:22.678979Z","shell.execute_reply":"2024-04-07T20:12:22.686487Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"### Formatting the Dataset\n\nNow, let's format the dataset in the required [Mistral-7B-Instruct-v0.1 format](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n\n> Many tutorials and blogs skip over this part, but I feel this is a really important step.\n\nWe'll put each instruction and input pair between `[INST]` and `[/INST]` output after that, like this:\n\n```\n<s>[INST] What is your favorite condiment? [/INST]\nWell, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!</s>\n```\n\nYou can use the following code to process your dataset and create a JSONL file in the correct format:","metadata":{"id":"Ey-cPKtDwEB-","papermill":{"duration":0.050535,"end_time":"2024-04-06T20:27:47.047952","exception":false,"start_time":"2024-04-06T20:27:46.997417","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def generate_prompt(row):\n    \"\"\"Generate input text based on a prompt, task instruction, and answer.\n\n#     :param row: Series: Data point (row of a pandas DataFrame)\n#     :return: str: generated prompt text\n#     \"\"\"\n    # Assuming 'prompt' is the generated context and 'rewrite_prompt' is the instruction\n    return f\"<s>[INST]{row['prompt']}[/INST]</s>\"\n","metadata":{"id":"Mjgn9ptNTrw8","papermill":{"duration":26.79163,"end_time":"2024-04-06T20:28:13.887812","exception":false,"start_time":"2024-04-06T20:27:47.096182","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-07T20:12:22.688533Z","iopub.execute_input":"2024-04-07T20:12:22.688883Z","iopub.status.idle":"2024-04-07T20:12:22.700714Z","shell.execute_reply.started":"2024-04-07T20:12:22.688857Z","shell.execute_reply":"2024-04-07T20:12:22.699600Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"## Step 3 - Inference","metadata":{}},{"cell_type":"code","source":"import re\n# def remove_special_characters(text):\n#     # This regex will match any character that is not a letter, number, or whitespace\n#     pattern = r'[^a-zA-Z0-9\\s]'\n#     text =text.replace(\"Transform\" ,\"improve\")\n#     text =text.replace(\"Reimagine\" ,\"rewrite\")\n#     # Replace these characters with an empty string\n#     clean_text = re.sub(pattern, '', text)\n#     return clean_text\n\ndef get_completion(prompt) -> str:\n    device = \"cuda:0\"\n    print('prompt')\n    print(prompt)\n    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\n    model_inputs = encodeds.to(device)\n\n    generated_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    print('gen')\n    print(generated_text)\n    prompt_length = len(tokenizer.encode(prompt))\n    print('output')\n    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n    generated_tokens = generated_ids[0][len(prompt_tokens):]\n    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n    print(generated_text)\n    return generated_text","metadata":{"id":"32dee9bf","papermill":{"duration":0.062529,"end_time":"2024-04-06T20:29:54.795350","exception":false,"start_time":"2024-04-06T20:29:54.732821","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-07T20:12:22.702023Z","iopub.execute_input":"2024-04-07T20:12:22.702376Z","iopub.status.idle":"2024-04-07T20:12:22.713388Z","shell.execute_reply.started":"2024-04-07T20:12:22.702347Z","shell.execute_reply":"2024-04-07T20:12:22.712409Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\ntest_df = pd.read_csv(TEST_CSV_PATH)\ntest_df['prompt'] = test_df.apply(lambda row: create_prompt(row['original_text'], row['rewritten_text']), axis=1)\ntest_df['prompt'] = test_df.apply(generate_prompt, axis=1)\ntest_df['rewrite_prompt'] = test_df.apply(lambda row: get_completion(row['prompt']), axis=1)\ntest_df = test_df[['id', 'rewrite_prompt']]\ntest_df","metadata":{"id":"c62b9142","outputId":"acb8e8d5-60a2-4e5a-a26e-8535be4ab614","papermill":{"duration":269.930823,"end_time":"2024-04-06T20:34:24.776384","exception":false,"start_time":"2024-04-06T20:29:54.845561","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-07T20:12:22.714973Z","iopub.execute_input":"2024-04-07T20:12:22.715353Z","iopub.status.idle":"2024-04-07T20:13:08.522793Z","shell.execute_reply.started":"2024-04-07T20:12:22.715317Z","shell.execute_reply":"2024-04-07T20:13:08.521688Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"prompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\nRewritten Essay: Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be[/INST]</s>\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"gen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\nRewritten Essay: Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be[/INST] \"Transform the original essay about a text competition into a nursery rhyme-style verse, focusing on the goal of identifying the rewrite prompts for each text.\"\noutput\n\"Transform the original essay about a text competition into a nursery rhyme-style verse, focusing on the goal of identifying the rewrite prompts for each text.\"\nprompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: Understanding the intricate dance of subatomic particles unveils the fabric of the cosmos.\nRewritten Essay: Behold the cosmic ballet: Where particles tiny, in intricate play, Weave the universe's endless fray.[/INST]</s>\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"gen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: Understanding the intricate dance of subatomic particles unveils the fabric of the cosmos.\nRewritten Essay: Behold the cosmic ballet: Where particles tiny, in intricate play, Weave the universe's endless fray.[/INST]PROMPT: Rewrite the given sentence preserving its meaning but using figurative language to describe the movement and interconnectedness of subatomic particles in the universe.\noutput\nPROMPT: Rewrite the given sentence preserving its meaning but using figurative language to describe the movement and interconnectedness of subatomic particles in the universe.\nprompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: Economic theories evolve, mirroring the shifting sands of time, adapting to new realities.\nRewritten Essay: Like dunes under the desert sun, economic beliefs shift, morph, and turn, ever-changing under time's relentless churn.[/INST]</s>\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"gen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: Economic theories evolve, mirroring the shifting sands of time, adapting to new realities.\nRewritten Essay: Like dunes under the desert sun, economic beliefs shift, morph, and turn, ever-changing under time's relentless churn.[/INST] evoke an image of economic beliefs as fluid and dynamic, shaped by the passage of time and influenced by current realities, using vivid metaphors from nature.\noutput\nevoke an image of economic beliefs as fluid and dynamic, shaped by the passage of time and influenced by current realities, using vivid metaphors from nature.\nprompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The ancient library held countless tomes, a testament to humanity's unquenchable thirst for knowledge.\nRewritten Essay: In halls of whispers, bound by dust, the library's tomes echo mankind's undying knowledge lust.[/INST]</s>\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"gen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The ancient library held countless tomes, a testament to humanity's unquenchable thirst for knowledge.\nRewritten Essay: In halls of whispers, bound by dust, the library's tomes echo mankind's undying knowledge lust.[/INST] Imaginate transforming the original essay by employing a more poetic and metaphorical language to maintain the essence of humanity's insatiable desire for knowledge.\noutput\nImaginate transforming the original essay by employing a more poetic and metaphorical language to maintain the essence of humanity's insatiable desire for knowledge.\nprompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: Quantum computing, with its potential to revolutionize our world, remains shrouded in mystery and complexity.\nRewritten Essay: Quantum realms, vast and untamed, promise revolution, yet cloak their game in layers of enigma and arcane flame.[/INST]</s>\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"gen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: Quantum computing, with its potential to revolutionize our world, remains shrouded in mystery and complexity.\nRewritten Essay: Quantum realms, vast and untamed, promise revolution, yet cloak their game in layers of enigma and arcane flame.[/INST]\tPrompt: Transform the \"Quantum computing\" language to a more metaphorical and poetic tone while maintaining the themes of mystery, potential revolution, and complexity.\noutput\nPrompt: Transform the \"Quantum computing\" language to a more metaphorical and poetic tone while maintaining the themes of mystery, potential revolution, and complexity.\nprompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The art of fermentation, an age-old practice, transforms simple ingredients into culinary treasures.\nRewritten Essay: Through time's slow press, the humble grape becomes wine's caress, a transformation deep, almost divine.[/INST]</s>\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"gen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The art of fermentation, an age-old practice, transforms simple ingredients into culinary treasures.\nRewritten Essay: Through time's slow press, the humble grape becomes wine's caress, a transformation deep, almost divine.[/INST]\"Transform the description of the art of fermentation into a metaphor involving time and the creation of a specific ingredient into a precious product.\"\noutput\n\"Transform the description of the art of fermentation into a metaphor involving time and the creation of a specific ingredient into a precious product.\"\nprompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: In the heart of the forest, the ancient tree stands tall, a silent witness to centuries gone by.\nRewritten Essay: Where shadows gather, and time slows its crawl, there stands the elder tree, forest's immortal thrall.[/INST]</s>\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"gen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: In the heart of the forest, the ancient tree stands tall, a silent witness to centuries gone by.\nRewritten Essay: Where shadows gather, and time slows its crawl, there stands the elder tree, forest's immortal thrall.[/INST] \"Transform the original essay into a more poetic and enchanting version by using imagery that emphasizes the tree's age, mystery, and connection to the forest.\"\noutput\n\"Transform the original essay into a more poetic and enchanting version by using imagery that emphasizes the tree's age, mystery, and connection to the forest.\"\nprompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The exploration of Mars opens new frontiers, challenging humanity to dream bigger.\nRewritten Essay: Mars beckons, a red jewel in the night, daring us to dream, to reach new heights.[/INST]</s>\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"gen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The exploration of Mars opens new frontiers, challenging humanity to dream bigger.\nRewritten Essay: Mars beckons, a red jewel in the night, daring us to dream, to reach new heights.[/INST]\"Write an essay that encourages a sense of wonder and exploration towards Mars, using descriptive language and an inviting tone.\"\noutput\n\"Write an essay that encourages a sense of wonder and exploration towards Mars, using descriptive language and an inviting tone.\"\nprompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The deep ocean, Earth's final frontier, teems with life, much of which remains to be discovered.\nRewritten Essay: Beneath the waves, in silent, dark embrace, lies a world unseen, an uncharted place.[/INST]</s>\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"gen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The deep ocean, Earth's final frontier, teems with life, much of which remains to be discovered.\nRewritten Essay: Beneath the waves, in silent, dark embrace, lies a world unseen, an uncharted place.[/INST] prospect: Transform the original essay's description of the deep ocean as Earth's final frontier to a more mysterious and unexplored tone by using metaphors and allusions to the hidden and unknown world beneath the waves.\noutput\nprospect: Transform the original essay's description of the deep ocean as Earth's final frontier to a more mysterious and unexplored tone by using metaphors and allusions to the hidden and unknown world beneath the waves.\nprompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: Artificial intelligence, once the realm of science fiction, now shapes the fabric of daily life.\nRewritten Essay: From the pages of fiction to the core of our days, AI weaves its threads, in unseen ways.[/INST]</s>\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"gen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: Artificial intelligence, once the realm of science fiction, now shapes the fabric of daily life.\nRewritten Essay: From the pages of fiction to the core of our days, AI weaves its threads, in unseen ways.[/INST] \"Transform the original essay's metaphor from Artificial Intelligence being a shaper to Artificial Intelligence being a weaver in the fabric of daily life\"\noutput\n\"Transform the original essay's metaphor from Artificial Intelligence being a shaper to Artificial Intelligence being a weaver in the fabric of daily life\"\nprompt\n<s>[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The migration of monarch butterflies, a journey across generations, is nature's marvel.\nRewritten Essay: In fluttering waves, across skies wide, monarchs traverse time, nature's guide.[/INST]</s>\ngen\n[INST]Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\nYou are trying to understand how the original essay was transformed into a new version.\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\nOutput should be only line ONLY\nOriginal Essay: The migration of monarch butterflies, a journey across generations, is nature's marvel.\nRewritten Essay: In fluttering waves, across skies wide, monarchs traverse time, nature's guide.[/INST] PROMPT: Transform the Original Essay's descriptive tone into a more poetic and metaphorical style.\noutput\nPROMPT: Transform the Original Essay's descriptive tone into a more poetic and metaphorical style.\n","output_type":"stream"},{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"    id                                     rewrite_prompt\n0   -1  \"Transform the original essay about a text com...\n1    0  PROMPT: Rewrite the given sentence preserving ...\n2    1  evoke an image of economic beliefs as fluid an...\n3    2  Imaginate transforming the original essay by e...\n4    3  Prompt: Transform the \"Quantum computing\" lang...\n5    4  \"Transform the description of the art of ferme...\n6    5  \"Transform the original essay into a more poet...\n7    6  \"Write an essay that encourages a sense of won...\n8    7  prospect: Transform the original essay's descr...\n9    8  \"Transform the original essay's metaphor from ...\n10   9  PROMPT: Transform the Original Essay's descrip...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>rewrite_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>\"Transform the original essay about a text com...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>PROMPT: Rewrite the given sentence preserving ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>evoke an image of economic beliefs as fluid an...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Imaginate transforming the original essay by e...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>Prompt: Transform the \"Quantum computing\" lang...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>\"Transform the description of the art of ferme...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>\"Transform the original essay into a more poet...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>\"Write an essay that encourages a sense of won...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7</td>\n      <td>prospect: Transform the original essay's descr...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>\"Transform the original essay's metaphor from ...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>9</td>\n      <td>PROMPT: Transform the Original Essay's descrip...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)","metadata":{"id":"2bfcdedb","papermill":{"duration":0.061158,"end_time":"2024-04-06T20:34:24.887979","exception":false,"start_time":"2024-04-06T20:34:24.826821","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-07T20:13:08.523982Z","iopub.execute_input":"2024-04-07T20:13:08.524285Z","iopub.status.idle":"2024-04-07T20:13:08.530580Z","shell.execute_reply.started":"2024-04-07T20:13:08.524258Z","shell.execute_reply":"2024-04-07T20:13:08.529563Z"},"trusted":true},"execution_count":79,"outputs":[]}]}