{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c98aa8a5",
   "metadata": {},
   "source": [
    "## Notebook for inference in the competition\n",
    "Credit: \n",
    "https://www.kaggle.com/code/khan1803115/llm-prompt-recovery-solution-lb-0-62 \n",
    "https://www.kaggle.com/code/isrswsiser/mistral-7b-prompt-recovery-adaptermodel/notebook\n",
    "\n",
    "This was used for submitting to the  LLM Prompt Recovery competition. Adapter model trained in Colab using the other notebook is used with Mistral-7b-it-v02 base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fc0f5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:04:24.786261Z",
     "iopub.status.busy": "2024-04-16T23:04:24.785903Z",
     "iopub.status.idle": "2024-04-16T23:04:24.790530Z",
     "shell.execute_reply": "2024-04-16T23:04:24.789700Z"
    },
    "id": "5izQNkEx5H54",
    "papermill": {
     "duration": 0.014846,
     "end_time": "2024-04-16T23:04:24.792630",
     "exception": false,
     "start_time": "2024-04-16T23:04:24.777784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Click on \"File + Add Input\"\n",
    "# Search for the dataset named \"llm-prompt-recovery\"\n",
    "# Once you find it, click on \"Add\" to attach it to your Kernel\n",
    "ADAPTER_MODEL_PATH = '/kaggle/input/llm-prompt-recovery-80'\n",
    "TEST_CSV_PATH = '/kaggle/input/llm-prompt-recovery/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734e2f54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:04:24.807624Z",
     "iopub.status.busy": "2024-04-16T23:04:24.806947Z",
     "iopub.status.idle": "2024-04-16T23:04:38.220323Z",
     "shell.execute_reply": "2024-04-16T23:04:38.219256Z"
    },
    "id": "7e-2gG8_5H54",
    "papermill": {
     "duration": 13.423368,
     "end_time": "2024-04-16T23:04:38.222781",
     "exception": false,
     "start_time": "2024-04-16T23:04:24.799413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#All dependencies are installed from Kaggle datasets\n",
    "#Credit:https://www.kaggle.com/code/hotchpotch/llm-detect-pip\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0941d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:04:38.237802Z",
     "iopub.status.busy": "2024-04-16T23:04:38.237499Z",
     "iopub.status.idle": "2024-04-16T23:05:50.660012Z",
     "shell.execute_reply": "2024-04-16T23:05:50.659074Z"
    },
    "id": "3nNWXXc7ol1n",
    "papermill": {
     "duration": 72.432751,
     "end_time": "2024-04-16T23:05:50.662459",
     "exception": false,
     "start_time": "2024-04-16T23:04:38.229708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/peft-whl-latest/peft-0.10.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (2.1.2)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (4.38.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (4.66.1)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (0.28.0)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (0.4.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.0) (0.21.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2024.3.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.10.0) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.10.0) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.10.0) (0.15.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.10.0) (1.3.0)\r\n",
      "Installing collected packages: peft\r\n",
      "Successfully installed peft-0.10.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U accelerate --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/transformers-4-38-2/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install /kaggle/input/peft-whl-latest/peft-0.10.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c4f5e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:05:50.679985Z",
     "iopub.status.busy": "2024-04-16T23:05:50.679636Z",
     "iopub.status.idle": "2024-04-16T23:06:02.913321Z",
     "shell.execute_reply": "2024-04-16T23:06:02.912099Z"
    },
    "id": "d9YXVCUi5H55",
    "papermill": {
     "duration": 12.245595,
     "end_time": "2024-04-16T23:06:02.915847",
     "exception": false,
     "start_time": "2024-04-16T23:05:50.670252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U peft --no-index --find-links ../input/llm-pkg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c23d45e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:06:02.933613Z",
     "iopub.status.busy": "2024-04-16T23:06:02.933276Z",
     "iopub.status.idle": "2024-04-16T23:06:09.841258Z",
     "shell.execute_reply": "2024-04-16T23:06:09.840429Z"
    },
    "id": "jO4ImqKE5H55",
    "papermill": {
     "duration": 6.919106,
     "end_time": "2024-04-16T23:06:09.843753",
     "exception": false,
     "start_time": "2024-04-16T23:06:02.924647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1173e78",
   "metadata": {},
   "source": [
    "## Model loading\n",
    "Mistral-7b instruction tuned model v02 seemed to be the best model for this task, it's loaded with quantization library BitsAndBytesConfig to reduce the usage of memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947cf37d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:06:09.874730Z",
     "iopub.status.busy": "2024-04-16T23:06:09.874279Z",
     "iopub.status.idle": "2024-04-16T23:07:58.640598Z",
     "shell.execute_reply": "2024-04-16T23:07:58.639643Z"
    },
    "id": "kvvLg99Opw5R",
    "papermill": {
     "duration": 108.777181,
     "end_time": "2024-04-16T23:07:58.643121",
     "exception": false,
     "start_time": "2024-04-16T23:06:09.865940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55416f65a0ed4cf1a8dce82bfbb11ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model(Mistral 7B)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    " )\n",
    "\n",
    "model_id = \"/kaggle/input/mistral-7b-it-v02\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9661b752",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:07:58.660183Z",
     "iopub.status.busy": "2024-04-16T23:07:58.659699Z",
     "iopub.status.idle": "2024-04-16T23:07:59.942508Z",
     "shell.execute_reply": "2024-04-16T23:07:59.941655Z"
    },
    "id": "ghK_9dnz87GR",
    "papermill": {
     "duration": 1.293634,
     "end_time": "2024-04-16T23:07:59.944733",
     "exception": false,
     "start_time": "2024-04-16T23:07:58.651099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Final adapter model used is from the other notebook training\n",
    "from peft import PeftConfig, PeftModel\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d5bbe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:07:59.961656Z",
     "iopub.status.busy": "2024-04-16T23:07:59.961362Z",
     "iopub.status.idle": "2024-04-16T23:07:59.976067Z",
     "shell.execute_reply": "2024-04-16T23:07:59.974847Z"
    },
    "id": "fsJ1HpYn5H55",
    "papermill": {
     "duration": 0.025521,
     "end_time": "2024-04-16T23:07:59.978156",
     "exception": false,
     "start_time": "2024-04-16T23:07:59.952635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45309bcc",
   "metadata": {},
   "source": [
    "# Getting the correct output \n",
    "Now necessary functions are defined to get suitable outputs from the model\n",
    "Credit: https://www.kaggle.com/code/khan1803115/llm-prompt-recovery-solution-lb-0-62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5c88e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:08:00.019483Z",
     "iopub.status.busy": "2024-04-16T23:08:00.019232Z",
     "iopub.status.idle": "2024-04-16T23:08:00.029713Z",
     "shell.execute_reply": "2024-04-16T23:08:00.028896Z"
    },
    "id": "21b408ac",
    "papermill": {
     "duration": 0.021392,
     "end_time": "2024-04-16T23:08:00.031661",
     "exception": false,
     "start_time": "2024-04-16T23:08:00.010269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_numbered_list(text):\n",
    "    final_text_paragraphs = []\n",
    "    for line in text.split('\\n'):\n",
    "        # Split each line at the first occurrence of '. '\n",
    "        parts = line.split('. ', 1)\n",
    "        # If the line looks like a numbered list item, remove the numbering\n",
    "        if len(parts) > 1 and parts[0].isdigit():\n",
    "            final_text_paragraphs.append(parts[1])\n",
    "        else:\n",
    "            # If it doesn't look like a numbered list item, include the line as is\n",
    "            final_text_paragraphs.append(line)\n",
    "\n",
    "    return '  '.join(final_text_paragraphs)\n",
    "\n",
    "\n",
    "#trims LLM output to just the response\n",
    "def trim_to_response(text):\n",
    "    terminate_string = \"[/INST]\"\n",
    "    text = text.replace('</s>', '')\n",
    "    #just in case it puts things in quotes\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "\n",
    "    last_pos = text.rfind(terminate_string)\n",
    "    return text[last_pos + len(terminate_string):] if last_pos != -1 else text\n",
    "\n",
    "#looks for response_start / returns only text that occurs after\n",
    "def extract_text_after_response_start(full_text):\n",
    "    parts = full_text.rsplit(response_start, 1)  # Split from the right, ensuring only the last occurrence is considered\n",
    "    if len(parts) > 1:\n",
    "        return parts[1].strip()  # Return text after the last occurrence of response_start\n",
    "    else:\n",
    "        return full_text  # Return the original text if response_start is not found\n",
    "\n",
    "\n",
    "#trims text to requested number of sentences (or first LF or double-space sequence)\n",
    "def trim_to_first_x_sentences_or_lf(text, x):\n",
    "    if x <= 0:\n",
    "        return \"\"\n",
    "\n",
    "    #any double-spaces dealt with as linefeed\n",
    "    text = text.replace(\"  \", \"\\n\")\n",
    "\n",
    "    text_chunks = text.split('\\n', 1)\n",
    "    first_chunk = text_chunks[0]\n",
    "    sentences = first_chunk.split('.')\n",
    "\n",
    "    if len(sentences) - 1 <= x:\n",
    "        trimmed_text = first_chunk\n",
    "    else:\n",
    "        # Otherwise, return the first x sentences\n",
    "        trimmed_text = '.'.join(sentences[:x]).strip()\n",
    "\n",
    "    if not trimmed_text.endswith('.'):\n",
    "        trimmed_text += '.'  # Add back the final period if the text chunk ended with one and was trimmed\n",
    "\n",
    "    return trimmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e593394f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:08:00.049047Z",
     "iopub.status.busy": "2024-04-16T23:08:00.048785Z",
     "iopub.status.idle": "2024-04-16T23:08:00.053980Z",
     "shell.execute_reply": "2024-04-16T23:08:00.053170Z"
    },
    "papermill": {
     "duration": 0.01602,
     "end_time": "2024-04-16T23:08:00.055861",
     "exception": false,
     "start_time": "2024-04-16T23:08:00.039841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#original text prefix\n",
    "orig_prefix = \"Original Text:\"\n",
    "\n",
    "#mistral \"response\"\n",
    "llm_response_for_rewrite = \"Provide the new text and I will tell you what new element was added or change in tone was made to improve it - with no references to the original.  I will avoid mentioning names of characters.  It is crucial no person, place or thing from the original text be mentioned.  For example - I will not say things like 'change the puppet show into a book report' - I would just say 'improve this text into a book report'.  If the original text mentions a specific idea, person, place, or thing - I will not mention it in my answer.  For example if there is a 'dog' or 'office' in the original text - the word 'dog' or 'office' must not be in my response.  My answer will be a single sentence.\"\n",
    "\n",
    "#modified text prefix\n",
    "rewrite_prefix = \"Re-written Text:\"\n",
    "\n",
    "#provided as start of Mistral response (anything after this is used as the prompt)\n",
    "#providing this as the start of the response helps keep things relevant\n",
    "response_start = \"The request was: \"\n",
    "\n",
    "#added after response_start to prime mistral\n",
    "#\"Improve this\" or \"Improve this text\" resulted in non-answers.\n",
    "#\"Improve this text by\" seems to product good results\n",
    "response_prefix = \"Improve this text by\"\n",
    "\n",
    "#well-scoring baseline text\n",
    "#thanks to: https://www.kaggle.com/code/rdxsun/lb-0-61\n",
    "base_line = 'Refine the following passage by emulating the writing style of [insert desired style here], with a focus on enhancing its clarity, elegance, and overall impact. Preserve the essence and original meaning of the text, while meticulously adjusting its tone, vocabulary, and stylistic elements to resonate with the chosen style.Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb554bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:08:00.073728Z",
     "iopub.status.busy": "2024-04-16T23:08:00.073469Z",
     "iopub.status.idle": "2024-04-16T23:08:00.087992Z",
     "shell.execute_reply": "2024-04-16T23:08:00.087172Z"
    },
    "papermill": {
     "duration": 0.026032,
     "end_time": "2024-04-16T23:08:00.089876",
     "exception": false,
     "start_time": "2024-04-16T23:08:00.063844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multiple examples were tried out from the original notebook and adding a few from prompting Gemma and from the dataset\n",
    "# While few-shot prompting definitely helped, at some point more examples didn't provide extra performance\n",
    "examples_sequences = [\n",
    "    (\n",
    "        \"Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite!\",\n",
    "        \"Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.\",\n",
    "        \"Improve this text to be a warning.\"\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.\",\n",
    "        \"Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.\",\n",
    "        \"Improve this text to make it a rap.\"\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"Drinking enough water each day is crucial for many functions in the body, such as regulating temperature, keeping joints lubricated, preventing infections, delivering nutrients to cells, and keeping organs functioning properly. Being well-hydrated also improves sleep quality, cognition, and mood.\",\n",
    "        \"Arrr, crew! Sail the health seas with water, the ultimate treasure! It steadies yer body's ship, fights off plagues, and keeps yer mind sharp. Hydrate or walk the plank into the abyss of ill health. Let's hoist our bottles high and drink to the horizon of well-being!\",\n",
    "        \"Improve this text to have a pirate.\"\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"In a bustling cityscape, under the glow of neon signs, Anna found herself at the crossroads of endless possibilities. The night was young, and the streets hummed with the energy of life. Drawn by the allure of the unknown, she wandered through the maze of alleys and boulevards, each turn revealing a new facet of the city's soul. It was here, amidst the symphony of urban existence, that Anna discovered the magic hidden in plain sight, the stories and dreams that thrived in the shadows of skyscrapers.\",\n",
    "        \"On an ordinary evening, amidst the cacophony of a neon-lit city, Anna stumbled upon an anomaly - a door that defied the laws of time and space. With the curiosity of a cat, she stepped through, leaving the familiar behind. Suddenly, she was adrift in the stream of time, witnessing the city's transformation from past to future, its buildings rising and falling like the breaths of a sleeping giant.\",\n",
    "        \"Improve this text by making it about time travel.\"\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"Late one night in the research lab, Dr. Evelyn Archer was on the brink of a breakthrough in artificial intelligence. Her fingers danced across the keyboard, inputting the final commands into the system. The lab was silent except for the hum of machinery and the occasional beep of computers. It was in this quiet orchestra of technology that Evelyn felt most at home, on the cusp of unveiling a creation that could change the world.\",\n",
    "        \"In the deep silence of the lab, under the watchful gaze of the moon, Dr. Evelyn Archer found herself not alone. Beside her, the iconic red eye of HAL 9000 flickered to life, a silent partner in her nocturnal endeavor. 'Good evening, Dr. Archer,' HAL's voice filled the room, devoid of warmth yet comforting in its familiarity. Together, they were about to initiate a test that would intertwine the destiny of human and artificial intelligence forever. As Evelyn entered the final command, HAL processed the data with unparalleled precision, a testament to the dawn of a new era.\",\n",
    "        \"Improve this text by adding an intelligent computer.\"\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"The park was empty, save for a solitary figure sitting on a bench, lost in thought. The quiet of the evening was punctuated only by the occasional rustle of leaves, offering a moment of peace in the chaos of city life.\",\n",
    "        \"Beneath the cloak of twilight, the park transformed into a realm of solitude and reflection. There, seated upon an ancient bench, was a lone soul, a guardian of secrets, enveloped in the serenity of nature's whispers. The dance of the leaves in the gentle breeze sang a lullaby to the tumult of the urban heart.\",\n",
    "        \"Improve this text to be more poetic.\"\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"The annual town fair was bustling with activity, from the merry-go-round spinning with laughter to the game booths challenging eager participants. Amidst the excitement, a figure in a cloak moved silently, almost invisibly, among the crowd, observing everything with keen interest but participating in none.\",\n",
    "        \"Beneath the riot of color and sound that marked the town's annual fair, a solitary figure roamed, known to the few as Eldrin the Enigmatic. Clad in a cloak that shimmered with the whispers of the arcane, Eldrin moved with the grace of a shadow, his gaze piercing the veneer of festivity to the magic beneath. As a master of the mystic arts, he sought not the laughter of the crowds but the silent stories woven into the fabric of the fair. With a flick of his wrist, he could coax wonder from the mundane, transforming the ordinary into spectacles of shimmering illusion, his true participation hidden within the folds of mystery.\",\n",
    "        \"Improve this text by adding a magician.\"\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"The startup team sat in the dimly lit room, surrounded by whiteboards filled with ideas, charts, and plans. They were on the brink of launching a new app designed to make home maintenance effortless for homeowners. The app would connect users with local service providers, using a sophisticated algorithm to match needs with skills and availability. As they debated the features and marketing strategies, the room felt charged with the energy of creation and the anticipation of what was to come.\",\n",
    "        \"In the quiet before dawn, a small group of innovators gathered, their mission: to simplify home maintenance through technology. But their true journey began with the unexpected addition of Max, a talking car with a knack for solving problems. 'Let me guide you through this maze of decisions,' Max offered, his dashboard flickering to life.\",\n",
    "        \"Improve this text by adding a talking car.\"\n",
    "    ),\n",
    "    (\n",
    "        \"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\",\n",
    "        \"Sure, here's the shanty: The competition's a-sailin', a text game so grand, The Gemma LLM be rewritin' the land. With rewrite prompts, we set sail, Unveilin' the secrets that lay at the trail. The text's a-changin', a rewrite we discern, From one prompt to the next, a tale to be born. The competition's a-blaze, a challenge to crave, Let us decipher the prompts that make the waves roar. So raise your mugs high as we sing this song, To the lingo of language, where words belong. The competition's a-sailin', a lyrical quest, Unraveling the threads that the language must.\",\n",
    "        \"Rewrite this text as a shanty while keeping the meaning, don't add anything else.\"\n",
    "    ),\n",
    "    (\"Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite!\"\n",
    "    ,\"Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.\"\n",
    "    ,\"Shift the narrative from offering an informal, comforting assurance to providing a more structured advisory note that emphasizes mindfulness and preparedness. This modification should mark a clear change in tone and purpose, aiming to responsibly inform and guide, while maintaining a positive and constructive approach.\"\n",
    "    ),\n",
    "    (\"A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.\"\n",
    "    ,\"Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.\"\n",
    "    ,\"Transform your communication from an academic delivery to a dynamic, rhythm-infused presentation. Keep the essence of the information intact but weave in artistic elements, utilizing rhythm, rhyme, and a conversational style. This approach should make the content more relatable and enjoyable, aiming to both educate and entertain your audience.\"\n",
    "    ),\n",
    "    (\"\"\"These guys are so busy fighting one another that they’re only continuing to facilitate the rise of Trump and Cruz,” said Representative Tom Cole of Oklahoma, a longtime Republican strategist.\n",
    "\n",
    "After the Iowa caucuses, and then the New Hampshire primary on Feb. 9, the pressure on those candidates who are lagging in the polls will intensify.\n",
    "\n",
    "“Whoever is not named Trump and not named Cruz that looks strong out of both Iowa and New Hampshire, we should consolidate around,” said Henry Barbour, an influential Republican strategist based in Mississippi.\n",
    "\n",
    "But it is not clear that the Iowa and New Hampshire results will come so neatly, or that the also-rans will be so willing to drop out after two states. Mr. Bush, in particular, still has the support of a well-funded super PAC. And even if he were to trail Mr. Rubio after the first contests, Mr. Bush might still fight on to South Carolina, where he recently won the support of Senator Lindsey Graham and where he plans to call on his older brother, former President George W. Bush.\n",
    "\n",
    "Some in the party now concede that it might take until March or beyond for the Republican establishment to coalesce behind an alternative to the current front-runners. And that could be too late to catch Mr. Trump or Mr. Cruz.\"\"\", \"\"\"## Dubplate Raggae in the Digital City\n",
    "\n",
    "(Verse 1)\n",
    "Big fightin' in the streets, but ain't no peace in the heart\n",
    "Grewin' tension in the GOP, tearing apart\n",
    "Two big names on top, Trump and Cruz\n",
    "They're lockin' horns, ain't no room for truce\n",
    "\n",
    "(Chorus)\n",
    "The clock is ticking, the pressure's on\n",
    "The clock is ticking, the pressure's on\n",
    "Iowa and New Hampshire, the stage is set\n",
    "But the battle ain't over, not yet\n",
    "\n",
    "(Verse 2)\n",
    "Other candidates stuck in the mud\n",
    "They gotta climb out, or get stuck in the mud\n",
    "Bush still got his super PAC\n",
    "And he ain't afraid to fight back\n",
    "\n",
    "(Chorus)\n",
    "The clock is ticking, the pressure's on\n",
    "The clock is ticking, the pressure's on\n",
    "Iowa and New Hampshire, the stage is set\n",
    "But the battle ain't over, not yet\n",
    "\n",
    "(Bridge)\n",
    "It's gonna take a while, it's gonna be a fight\n",
    "Until the Republican party finds its light\n",
    "And if they wait too long, it's all done\n",
    "Trump and Cruz will be the ones\n",
    "\n",
    "(Chorus)\n",
    "The clock is ticking, the pressure's on\n",
    "The clock is ticking, the pressure's on\n",
    "Iowa and New Hampshire, the stage is set\n",
    "But the battle ain't over, not yet\n",
    "\n",
    "(Outro)\n",
    "So hold on tight, folks, it's gonna be a ride\n",
    "The Republican party ain't gonna be the same\n",
    "But through the struggle, there's one thing we know\n",
    "The future's bright, and the sky's the limit\"\"\", \"Imagine this text was a raggae song in a cyberpunk city, and Reconstruct it\")\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "313ef0fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:08:00.107063Z",
     "iopub.status.busy": "2024-04-16T23:08:00.106782Z",
     "iopub.status.idle": "2024-04-16T23:08:00.116807Z",
     "shell.execute_reply": "2024-04-16T23:08:00.115984Z"
    },
    "papermill": {
     "duration": 0.020812,
     "end_time": "2024-04-16T23:08:00.118785",
     "exception": false,
     "start_time": "2024-04-16T23:08:00.097973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main function for getting the prompt from the model\n",
    "# Different values for temperature, max_tokens and sentences were tried out but none caused massive differences\n",
    "def get_prompt(orig_text, transformed_text):\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    # Append example sequences\n",
    "    for example_text, example_rewrite, example_prompt in examples_sequences:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"{orig_prefix} {example_text}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llm_response_for_rewrite})\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"{rewrite_prefix} {example_rewrite}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"{response_start} {example_prompt}\"})\n",
    "\n",
    "    #actual prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{orig_prefix} {orig_text}\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": llm_response_for_rewrite})\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{rewrite_prefix} {transformed_text}\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{response_start} {response_prefix}\"})\n",
    "\n",
    "\n",
    "    tokenizer.padding = 'right'\n",
    "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    model_inputs = model_inputs.to(\"cuda\")\n",
    "    with  torch.no_grad() :\n",
    "        generated_ids = model.generate(model_inputs, max_new_tokens=100 , do_sample =True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    #decode and trim to actual response\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    just_response = trim_to_response(decoded[0])\n",
    "    final_text = extract_text_after_response_start(just_response)\n",
    "\n",
    "    #mistral has been replying with numbered lists - clean them up....\n",
    "    final_text = remove_numbered_list(final_text)\n",
    "\n",
    "    #mistral v02 tends to respond with the input after providing the answer - this tries to trim that down\n",
    "    final_text = trim_to_first_x_sentences_or_lf(final_text, 1)\n",
    "\n",
    "    #default to baseline if empty or unusually short\n",
    "    if len(final_text) < 15:\n",
    "        final_text = base_line\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd8951cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:08:00.135741Z",
     "iopub.status.busy": "2024-04-16T23:08:00.135467Z",
     "iopub.status.idle": "2024-04-16T23:08:00.140198Z",
     "shell.execute_reply": "2024-04-16T23:08:00.139372Z"
    },
    "id": "zKsHkqbG5NaJ",
    "papermill": {
     "duration": 0.015346,
     "end_time": "2024-04-16T23:08:00.142079",
     "exception": false,
     "start_time": "2024-04-16T23:08:00.126733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special_characters(text):\n",
    "    # This regex will match any character that is not a letter, number, or whitespace\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    text =text.replace(\"Transform\" ,\"improve\")\n",
    "    text =text.replace(\"Reimagine\" ,\"rewrite\")\n",
    "    # Replace these characters with an empty string\n",
    "    clean_text = re.sub(pattern, '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28302e61",
   "metadata": {
    "id": "PhXf2plq5H56",
    "papermill": {
     "duration": 0.008089,
     "end_time": "2024-04-16T23:08:00.223398",
     "exception": false,
     "start_time": "2024-04-16T23:08:00.215309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference\n",
    "Finally inference is done for the submission, where the model is prompted for each set of texts and outputs a possible prompt based on them using previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2024-04-16 23:08:04.536256: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-16 23:08:04.536392: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-16 23:08:04.661913: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improve this text by making it into a competitive shanty song while respecting the original context of the code competition\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Improve this text by making it into a competit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                     rewrite_prompt\n",
       "0  -1  Improve this text by making it into a competit..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
    "adject =[]\n",
    "for index, row in test_df.iterrows():\n",
    "        result = get_prompt(row['original_text'], row['rewritten_text'])\n",
    "        result  = remove_special_characters(result)\n",
    "        print(result)\n",
    "        test_df.at[index, 'rewrite_prompt'] =  result\n",
    "\n",
    "test_df = test_df[['id', 'rewrite_prompt']]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60d5599e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T23:08:42.509214Z",
     "iopub.status.busy": "2024-04-16T23:08:42.508603Z",
     "iopub.status.idle": "2024-04-16T23:08:42.517275Z",
     "shell.execute_reply": "2024-04-16T23:08:42.516492Z"
    },
    "id": "2bfcdedb",
    "papermill": {
     "duration": 0.020201,
     "end_time": "2024-04-16T23:08:42.519267",
     "exception": false,
     "start_time": "2024-04-16T23:08:42.499066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7806901,
     "sourceId": 67121,
     "sourceType": "competition"
    },
    {
     "datasetId": 3574875,
     "sourceId": 6229341,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4599344,
     "sourceId": 7844455,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4634330,
     "sourceId": 7893017,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4593930,
     "sourceId": 7925072,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4713888,
     "sourceId": 8004247,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4750212,
     "sourceId": 8057902,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4766765,
     "sourceId": 8077021,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4770771,
     "sourceId": 8082552,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4771938,
     "sourceId": 8084126,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4778609,
     "sourceId": 8093729,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4804992,
     "sourceId": 8129695,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 171590944,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 264.276091,
   "end_time": "2024-04-16T23:08:45.286213",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-16T23:04:21.010122",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "14d7c8aa62da4d4f8325cecec9e108d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b95d0c281e01443e96040e2eb9ed4905",
       "max": 3,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d54f708154ba4d44b26573df275a5340",
       "value": 3
      }
     },
     "2dd87163f4bc446888d6d522e0f0fed7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "336ae332d07644dab65672f1014dfdbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7969b850c4fd4273b8c0654c14026f92",
       "placeholder": "​",
       "style": "IPY_MODEL_4cd19b1391f94a82b9ecde4c537de2e4",
       "value": " 3/3 [01:45&lt;00:00, 34.77s/it]"
      }
     },
     "4cd19b1391f94a82b9ecde4c537de2e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "55416f65a0ed4cf1a8dce82bfbb11ec7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6411cb4f99024d2cab77969bfa51838e",
        "IPY_MODEL_14d7c8aa62da4d4f8325cecec9e108d5",
        "IPY_MODEL_336ae332d07644dab65672f1014dfdbf"
       ],
       "layout": "IPY_MODEL_2dd87163f4bc446888d6d522e0f0fed7"
      }
     },
     "6411cb4f99024d2cab77969bfa51838e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bc2c653b0243483c96ea6545d58a97f6",
       "placeholder": "​",
       "style": "IPY_MODEL_95d3f594905b47828c734202deaf16f6",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "7969b850c4fd4273b8c0654c14026f92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "95d3f594905b47828c734202deaf16f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b95d0c281e01443e96040e2eb9ed4905": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc2c653b0243483c96ea6545d58a97f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d54f708154ba4d44b26573df275a5340": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
