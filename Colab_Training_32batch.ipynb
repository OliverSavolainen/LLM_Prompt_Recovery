{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF1zDzaQvfKL"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before delving into the fine-tuning process, ensure that you have the following prerequisites in place:\n",
        "\n",
        "1. **GPU**: This tutorial cannot run on free Google Colab; it requires more powerful GPUs, such as the A100.\n",
        "2. **Python Packages**: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:\n",
        "\n",
        "Let's begin by checking if your GPU is correctly detected:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arrkM1d69rGK"
      },
      "source": [
        "Let's define a wrapper function which will get completion from the model from a user question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4CGt_EezL7n"
      },
      "source": [
        "Let's define a wrapper function which will get completion from the model from a user question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSqkDaANytW7"
      },
      "source": [
        "## Step 1 Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7pwtgaaG16f"
      },
      "source": [
        "### Step 1.1 - Set variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LG7H_D7CHD5x"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = '/content/drive/My Drive/Colab Notebooks/model'\n",
        "MODEL_NAME = 'llm-prompt-recovery-mistral-6'\n",
        "CHECKPOINT_PATH = '/content/drive/My Drive/Colab Notebooks/checkpoints'\n",
        "\n",
        "PYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\"\n",
        "\n",
        "HUGGING_FACE_USERNAME = 'OliverSavolainen'\n",
        "HUGGING_FACE_REPO_PATH = '/content/drive/My Drive/Colab Notebooks/huggingFaceRepo'\n",
        "# print(f'{HUGGING_FACE_USERNAME}/{MODEL_NAME}')\n",
        "\n",
        "TRAIN_FILE_PATH = '/content/drive/My Drive/Colab Notebooks/df_with_emb_20240402.csv'\n",
        "\n",
        "KAGGLE_CREDENTIALS_PATH = '/content/drive/My Drive/Colab Notebooks/kaggle.json'\n",
        "KAGGLE_DATASET_PATH = '/content/dataset'\n",
        "KAGGLE_DATASET_NAME = MODEL_NAME\n",
        "KAGGLE_USERNAME = HUGGING_FACE_USERNAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbu-rE_Hy40y"
      },
      "source": [
        "### Step 1.2 - Set Credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LKMc2Qe2sUib",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "d049fe24-b766-47f7-8025-ef2aef99dda3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh9I5dbasWKC"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "# login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1qQ3ty7BNgy"
      },
      "outputs": [],
      "source": [
        "# Setup Kaggle API Credentials:\n",
        "# Make sure kaggle.json file exists in this folder\n",
        "!mkdir ~/.kaggle\n",
        "!cp \"$KAGGLE_CREDENTIALS_PATH\" ~/.kaggle/\n",
        "!ls ~/.kaggle/\n",
        "\n",
        "# Change the permissions of the file.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfYrIMWazL7y"
      },
      "source": [
        "### Step 1.3 - Install necessary packages\n",
        "First, install the dependencies below to get started. As these features are available on the main branches only, we need to install the libraries below from source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nNWXXc7ol1n"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets scipy\n",
        "!pip install -q trl\n",
        "\n",
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3BShqhFlS3g"
      },
      "source": [
        "### 1.4 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ykIKvg-lRVZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftConfig, PeftModel\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "import bitsandbytes as bnb\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import transformers\n",
        "from trl import SFTTrainer\n",
        "import warnings\n",
        "from huggingface_hub import Repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NVvNhohkvwF"
      },
      "source": [
        "## Step 2 - Model loading\n",
        "We'll load the model using QLoRA quantization to reduce the usage of memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvvLg99Opw5R"
      },
      "outputs": [],
      "source": [
        "# Load base model(Mistral 7B)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= 'nf4',\n",
        "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant= False,\n",
        " )\n",
        "\n",
        "\n",
        "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghK_9dnz87GR"
      },
      "outputs": [],
      "source": [
        "adapter_model_name = '/content/drive/My Drive/Colab Notebooks/checkpoints/checkpoint-80/'\n",
        "MODEL_NAME = 'llm-prompt-recovery-mistral-6'\n",
        "model = PeftModel.from_pretrained(model, adapter_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgqxSuxsBX3r"
      },
      "source": [
        "Now we specify the model ID and then we load it with our previously defined quantization configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omw10c2djdIw"
      },
      "source": [
        "Run a inference on the base model. The model does not seem to understand our instruction and gives us a list of questions related to our query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m06rH8cTrZof"
      },
      "source": [
        "## Step 3 - Load dataset for finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6QLtYfrxD5"
      },
      "source": [
        "### Lets Load the Dataset\n",
        "\n",
        "For this tutorial, we will fine-tune Mistral 7B Instruct for code generation.\n",
        "\n",
        "We will be using this [dataset](https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style) which is curated by [TokenBender (e/xperiments)](https://twitter.com/4evaBehindSOTA) and is an excellent data source for fine-tuning models for code generation. It follows the alpaca style of instructions, which is an excellent starting point for this task. The dataset structure should resemble the following:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",\n",
        "  \"input\": \"[1, 2, 3, 4, 5]\",\n",
        "  \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-02T07:25:12.879589Z",
          "iopub.status.busy": "2024-04-02T07:25:12.879329Z",
          "iopub.status.idle": "2024-04-02T07:25:12.884304Z",
          "shell.execute_reply": "2024-04-02T07:25:12.883406Z"
        },
        "id": "ea15a5a5",
        "papermill": {
          "duration": 0.017375,
          "end_time": "2024-04-02T07:25:12.886070",
          "exception": false,
          "start_time": "2024-04-02T07:25:12.868695",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#original text prefix\n",
        "orig_prefix = \"Original Text:\"\n",
        "\n",
        "#mistral \"response\"\n",
        "llm_response_for_rewrite = \"Provide the modified text and I'll tell you a something general that changed about it.\"\n",
        "#modified text prefix\n",
        "rewrite_prefix = \"Re-written Text:\"\n",
        "\n",
        "#provided as start of Mistral response (anything after this is used as the prompt)\n",
        "#providing this as the start of the response helps keep things relevant\n",
        "response_start = \"The request was: \"\n",
        "\n",
        "#We insert our detected prompt into this well-scoring baseline text\n",
        "#thanks to: https://www.kaggle.com/code/rdxsun/lb-0-61\n",
        "base_line = 'Please improve the following text using the writing style of {}, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.'\n",
        "base_line_swap_text = \"[insert desired style here]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-02T07:25:12.924091Z",
          "iopub.status.busy": "2024-04-02T07:25:12.923830Z",
          "iopub.status.idle": "2024-04-02T07:25:12.929045Z",
          "shell.execute_reply": "2024-04-02T07:25:12.928179Z"
        },
        "id": "194af12e",
        "papermill": {
          "duration": 0.017423,
          "end_time": "2024-04-02T07:25:12.931044",
          "exception": false,
          "start_time": "2024-04-02T07:25:12.913621",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#first example\n",
        "example_text_1 = \"Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite!\"\n",
        "example_rewrite_1 = \"Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.\"\n",
        "example_prompt_1 = \"Shift the narrative from offering an informal, comforting assurance to providing a more structured advisory note that emphasizes mindfulness and preparedness. This modification should mark a clear change in tone and purpose, aiming to responsibly inform and guide, while maintaining a positive and constructive approach.\"\n",
        "\n",
        "#second example\n",
        "example_text_2 = \"A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.\"\n",
        "example_rewrite_2 = \"Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.\"\n",
        "example_prompt_2 = \"Transform your communication from an academic delivery to a dynamic, rhythm-infused presentation. Keep the essence of the information intact but weave in artistic elements, utilizing rhythm, rhyme, and a conversational style. This approach should make the content more relatable and enjoyable, aiming to both educate and entertain your audience.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-02T07:25:12.969524Z",
          "iopub.status.busy": "2024-04-02T07:25:12.968881Z",
          "iopub.status.idle": "2024-04-02T07:25:12.979630Z",
          "shell.execute_reply": "2024-04-02T07:25:12.978801Z"
        },
        "id": "21b408ac",
        "papermill": {
          "duration": 0.022508,
          "end_time": "2024-04-02T07:25:12.981489",
          "exception": false,
          "start_time": "2024-04-02T07:25:12.958981",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def remove_numbered_list(text):\n",
        "    final_text_paragraphs = []\n",
        "    for line in text.split('\\n'):\n",
        "        # Split each line at the first occurrence of '. '\n",
        "        parts = line.split('. ', 1)\n",
        "        # If the line looks like a numbered list item, remove the numbering\n",
        "        if len(parts) > 1 and parts[0].isdigit():\n",
        "            final_text_paragraphs.append(parts[1])\n",
        "        else:\n",
        "            # If it doesn't look like a numbered list item, include the line as is\n",
        "            final_text_paragraphs.append(line)\n",
        "\n",
        "    return '  '.join(final_text_paragraphs)\n",
        "\n",
        "\n",
        "#trims LLM output to just the response\n",
        "def trim_to_response(text):\n",
        "    terminate_string = \"[/INST]\"\n",
        "    text = text.replace('</s>', '')\n",
        "    #just in case it puts things in quotes\n",
        "    text = text.replace('\"', '')\n",
        "    text = text.replace(\"'\", '')\n",
        "\n",
        "    last_pos = text.rfind(terminate_string)\n",
        "    return text[last_pos + len(terminate_string):] if last_pos != -1 else text\n",
        "\n",
        "#looks for response_start / returns only text that occurs after\n",
        "def extract_text_after_response_start(full_text):\n",
        "    parts = full_text.rsplit(response_start, 1)  # Split from the right, ensuring only the last occurrence is considered\n",
        "    if len(parts) > 1:\n",
        "        return parts[1].strip()  # Return text after the last occurrence of response_start\n",
        "    else:\n",
        "        return full_text  # Return the original text if response_start is not found\n",
        "\n",
        "\n",
        "#trims text to requested number of sentences (or first LF or double-space sequence)\n",
        "def trim_to_first_x_sentences_or_lf(text, x):\n",
        "    if x <= 0:\n",
        "        return \"\"\n",
        "\n",
        "    #any double-spaces dealt with as linefeed\n",
        "    text = text.replace(\"  \", \"\\n\")\n",
        "\n",
        "    text_chunks = text.split('\\n', 1)\n",
        "    first_chunk = text_chunks[0]\n",
        "    sentences = first_chunk.split('.')\n",
        "\n",
        "    if len(sentences) - 1 <= x:\n",
        "        trimmed_text = first_chunk\n",
        "    else:\n",
        "        # Otherwise, return the first x sentences\n",
        "        trimmed_text = '.'.join(sentences[:x]).strip()\n",
        "\n",
        "    if not trimmed_text.endswith('.'):\n",
        "        trimmed_text += '.'  # Add back the final period if the text chunk ended with one and was trimmed\n",
        "\n",
        "    return trimmed_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#original text prefix\n",
        "orig_prefix = \"Original Text:\"\n",
        "\n",
        "#mistral \"response\"\n",
        "llm_response_for_rewrite = \"Provide the new text and I will tell you what new element was added or change in tone was made to improve it - with no references to the original.  I will avoid mentioning names of characters.  It is crucial no person, place or thing from the original text be mentioned.  For example - I will not say things like 'change the puppet show into a book report' - I would just say 'improve this text into a book report'.  If the original text mentions a specific idea, person, place, or thing - I will not mention it in my answer.  For example if there is a 'dog' or 'office' in the original text - the word 'dog' or 'office' must not be in my response.  My answer will be a single sentence.\"\n",
        "\n",
        "#modified text prefix\n",
        "rewrite_prefix = \"Re-written Text:\"\n",
        "\n",
        "#provided as start of Mistral response (anything after this is used as the prompt)\n",
        "#providing this as the start of the response helps keep things relevant\n",
        "response_start = \"The request was: \"\n",
        "\n",
        "#added after response_start to prime mistral\n",
        "#\"Improve this\" or \"Improve this text\" resulted in non-answers.\n",
        "#\"Improve this text by\" seems to product good results\n",
        "response_prefix = \"Improve this text by\"\n",
        "\n",
        "#well-scoring baseline text\n",
        "#thanks to: https://www.kaggle.com/code/rdxsun/lb-0-61\n",
        "base_line = 'Refine the following passage by emulating the writing style of [insert desired style here], with a focus on enhancing its clarity, elegance, and overall impact. Preserve the essence and original meaning of the text, while meticulously adjusting its tone, vocabulary, and stylistic elements to resonate with the chosen style.Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.'"
      ],
      "metadata": {
        "id": "NDM5VGZXs7AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "examples_sequences = [\n",
        "    (\n",
        "        \"Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite!\",\n",
        "        \"Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.\",\n",
        "        \"Improve this text to be a warning.\"\n",
        "    ),\n",
        "\n",
        "    (\n",
        "        \"A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.\",\n",
        "        \"Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.\",\n",
        "        \"Improve this text to make it a rap.\"\n",
        "    ),\n",
        "\n",
        "    (\n",
        "        \"Drinking enough water each day is crucial for many functions in the body, such as regulating temperature, keeping joints lubricated, preventing infections, delivering nutrients to cells, and keeping organs functioning properly. Being well-hydrated also improves sleep quality, cognition, and mood.\",\n",
        "        \"Arrr, crew! Sail the health seas with water, the ultimate treasure! It steadies yer body's ship, fights off plagues, and keeps yer mind sharp. Hydrate or walk the plank into the abyss of ill health. Let's hoist our bottles high and drink to the horizon of well-being!\",\n",
        "        \"Improve this text to have a pirate.\"\n",
        "    ),\n",
        "\n",
        "    (\n",
        "        \"In a bustling cityscape, under the glow of neon signs, Anna found herself at the crossroads of endless possibilities. The night was young, and the streets hummed with the energy of life. Drawn by the allure of the unknown, she wandered through the maze of alleys and boulevards, each turn revealing a new facet of the city's soul. It was here, amidst the symphony of urban existence, that Anna discovered the magic hidden in plain sight, the stories and dreams that thrived in the shadows of skyscrapers.\",\n",
        "        \"On an ordinary evening, amidst the cacophony of a neon-lit city, Anna stumbled upon an anomaly - a door that defied the laws of time and space. With the curiosity of a cat, she stepped through, leaving the familiar behind. Suddenly, she was adrift in the stream of time, witnessing the city's transformation from past to future, its buildings rising and falling like the breaths of a sleeping giant.\",\n",
        "        \"Improve this text by making it about time travel.\"\n",
        "    ),\n",
        "\n",
        "    (\n",
        "        \"Late one night in the research lab, Dr. Evelyn Archer was on the brink of a breakthrough in artificial intelligence. Her fingers danced across the keyboard, inputting the final commands into the system. The lab was silent except for the hum of machinery and the occasional beep of computers. It was in this quiet orchestra of technology that Evelyn felt most at home, on the cusp of unveiling a creation that could change the world.\",\n",
        "        \"In the deep silence of the lab, under the watchful gaze of the moon, Dr. Evelyn Archer found herself not alone. Beside her, the iconic red eye of HAL 9000 flickered to life, a silent partner in her nocturnal endeavor. 'Good evening, Dr. Archer,' HAL's voice filled the room, devoid of warmth yet comforting in its familiarity. Together, they were about to initiate a test that would intertwine the destiny of human and artificial intelligence forever. As Evelyn entered the final command, HAL processed the data with unparalleled precision, a testament to the dawn of a new era.\",\n",
        "        \"Improve this text by adding an intelligent computer.\"\n",
        "    ),\n",
        "\n",
        "    (\n",
        "        \"The park was empty, save for a solitary figure sitting on a bench, lost in thought. The quiet of the evening was punctuated only by the occasional rustle of leaves, offering a moment of peace in the chaos of city life.\",\n",
        "        \"Beneath the cloak of twilight, the park transformed into a realm of solitude and reflection. There, seated upon an ancient bench, was a lone soul, a guardian of secrets, enveloped in the serenity of nature's whispers. The dance of the leaves in the gentle breeze sang a lullaby to the tumult of the urban heart.\",\n",
        "        \"Improve this text to be more poetic.\"\n",
        "    ),\n",
        "\n",
        "    (\n",
        "        \"The annual town fair was bustling with activity, from the merry-go-round spinning with laughter to the game booths challenging eager participants. Amidst the excitement, a figure in a cloak moved silently, almost invisibly, among the crowd, observing everything with keen interest but participating in none.\",\n",
        "        \"Beneath the riot of color and sound that marked the town's annual fair, a solitary figure roamed, known to the few as Eldrin the Enigmatic. Clad in a cloak that shimmered with the whispers of the arcane, Eldrin moved with the grace of a shadow, his gaze piercing the veneer of festivity to the magic beneath. As a master of the mystic arts, he sought not the laughter of the crowds but the silent stories woven into the fabric of the fair. With a flick of his wrist, he could coax wonder from the mundane, transforming the ordinary into spectacles of shimmering illusion, his true participation hidden within the folds of mystery.\",\n",
        "        \"Improve this text by adding a magician.\"\n",
        "    ),\n",
        "\n",
        "    (\n",
        "        \"The startup team sat in the dimly lit room, surrounded by whiteboards filled with ideas, charts, and plans. They were on the brink of launching a new app designed to make home maintenance effortless for homeowners. The app would connect users with local service providers, using a sophisticated algorithm to match needs with skills and availability. As they debated the features and marketing strategies, the room felt charged with the energy of creation and the anticipation of what was to come.\",\n",
        "        \"In the quiet before dawn, a small group of innovators gathered, their mission: to simplify home maintenance through technology. But their true journey began with the unexpected addition of Max, a talking car with a knack for solving problems. 'Let me guide you through this maze of decisions,' Max offered, his dashboard flickering to life.\",\n",
        "        \"Improve this text by adding a talking car.\"\n",
        "    ),\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "sqOCfIhMs8ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt2(orig_text, transformed_text):\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    # Append example sequences\n",
        "    for example_text, example_rewrite, example_prompt in examples_sequences:\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"{orig_prefix} {example_text}\"})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": llm_response_for_rewrite})\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"{rewrite_prefix} {example_rewrite}\"})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": f\"{response_start} {example_prompt}\"})\n",
        "\n",
        "    #actual prompt\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"{orig_prefix} {orig_text}\"})\n",
        "    messages.append({\"role\": \"assistant\", \"content\": llm_response_for_rewrite})\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"{rewrite_prefix} {transformed_text}\"})\n",
        "    messages.append({\"role\": \"assistant\", \"content\": f\"{response_start} {response_prefix}\"})\n",
        "\n",
        "\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)"
      ],
      "metadata": {
        "id": "phPcK4lBtfsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-02T07:25:13.020122Z",
          "iopub.status.busy": "2024-04-02T07:25:13.019282Z",
          "iopub.status.idle": "2024-04-02T07:25:13.029548Z",
          "shell.execute_reply": "2024-04-02T07:25:13.028700Z"
        },
        "id": "77380d19",
        "papermill": {
          "duration": 0.022129,
          "end_time": "2024-04-02T07:25:13.031366",
          "exception": false,
          "start_time": "2024-04-02T07:25:13.009237",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_prompt(orig_text, transformed_text):\n",
        "\n",
        "    #construct the prompt...\n",
        "    messages = [\n",
        "\n",
        "        #first example\n",
        "        {\"role\": \"user\", \"content\": f\"{orig_prefix} {example_text_1}\"},\n",
        "        {\"role\": \"assistant\", \"content\": llm_response_for_rewrite},\n",
        "        {\"role\": \"user\", \"content\": f\"{rewrite_prefix} {example_rewrite_1}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"{response_start} {example_prompt_1}\"},\n",
        "\n",
        "        #second example\n",
        "        {\"role\": \"user\", \"content\": f\"{orig_prefix} {example_text_2}\"},\n",
        "        {\"role\": \"assistant\", \"content\": llm_response_for_rewrite},\n",
        "        {\"role\": \"user\", \"content\": f\"{rewrite_prefix} {example_rewrite_2}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"{response_start} {example_prompt_2}\"},\n",
        "\n",
        "        #actual prompt\n",
        "        {\"role\": \"user\", \"content\": f\"{orig_prefix} {orig_text}\"},\n",
        "        {\"role\": \"assistant\", \"content\": llm_response_for_rewrite},\n",
        "        {\"role\": \"user\", \"content\": f\"{rewrite_prefix} {transformed_text}\"},\n",
        "        {\"role\": \"assistant\", \"content\": response_start},\n",
        "    ]\n",
        "\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-02T07:25:17.180325Z",
          "iopub.status.busy": "2024-04-02T07:25:17.180047Z",
          "iopub.status.idle": "2024-04-02T07:25:17.184981Z",
          "shell.execute_reply": "2024-04-02T07:25:17.184105Z"
        },
        "id": "GfPSNnbI2TvN",
        "papermill": {
          "duration": 0.016839,
          "end_time": "2024-04-02T07:25:17.186883",
          "exception": false,
          "start_time": "2024-04-02T07:25:17.170044",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def remove_special_characters(text):\n",
        "    # This regex will match any character that is not a letter, number, or whitespace\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    text =text.replace(\"Transform\" ,\"improve\")\n",
        "    text =text.replace(\"Reimagine\" ,\"rewrite\")\n",
        "    # Replace these characters with an empty string\n",
        "    clean_text = re.sub(pattern, '', text)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(TRAIN_FILE_PATH)"
      ],
      "metadata": {
        "id": "CGDSybMR6UHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(df['dataset_id'].tolist())"
      ],
      "metadata": {
        "id": "IhPiPfByZOhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDH2h3FNmNiD"
      },
      "outputs": [],
      "source": [
        "df = df[df['dataset_id'] == 'nbroad_2']\n",
        "\n",
        "def create_prompt(original, rewritten):\n",
        "    return f\"Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\\nYou are trying to understand how the original essay was transformed into a new version.\\nAnalyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\\nOnly give me the PROMPT. Start directly with the prompt, that's all I need.\\nOutput should be only line ONLY\\nOriginal Essay: {original}\\nRewritten Essay: {rewritten}\"\n",
        "\n",
        "# Apply the function to each row to create the prompt column\n",
        "df['prompt'] = df.apply(lambda row: get_prompt2(row['original_text'], row['rewritten_text']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAP-jYBjrwUc"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2f5tr1-SJd6"
      },
      "source": [
        "Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :\n",
        "1. the function generate_prompt : take the instruction and output and generate a prompt\n",
        "2. shuffle the dataset\n",
        "3. tokenizer the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey-cPKtDwEB-"
      },
      "source": [
        "### Formatting the Dataset\n",
        "\n",
        "Now, let's format the dataset in the required [Mistral-7B-Instruct-v0.1 format](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n",
        "\n",
        "> Many tutorials and blogs skip over this part, but I feel this is a really important step.\n",
        "\n",
        "We'll put each instruction and input pair between `[INST]` and `[/INST]` output after that, like this:\n",
        "\n",
        "```\n",
        "<s>[INST] What is your favorite condiment? [/INST]\n",
        "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!</s>\n",
        "```\n",
        "\n",
        "You can use the following code to process your dataset and create a JSONL file in the correct format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjgn9ptNTrw8"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(row):\n",
        "    \"\"\"Generate input text based on a prompt, task instruction, and answer.\n",
        "\n",
        "    :param row: Series: Data point (row of a pandas DataFrame)\n",
        "    :return: str: generated prompt text\n",
        "    \"\"\"\n",
        "    # Assuming 'prompt' is the generated context and 'rewrite_prompt' is the instruction\n",
        "    return f\"<s>[INST]{row['prompt']} [/INST]{row['rewrite_prompt']}</s>\"\n",
        "\n",
        "# Apply the function to each row to transform the 'prompt' and 'rewrite_prompt' columns\n",
        "#df['prompt'] = df.apply(generate_prompt, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw8noUkUDbFY"
      },
      "outputs": [],
      "source": [
        "df = df[['prompt', 'rewrite_prompt']]\n",
        "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "#dataset = dataset.select(range(10)) # ToDo: remove that\n",
        "# Shuffle the dataset with a seed for reproducibility\n",
        "dataset = dataset.shuffle(seed=1234)\n",
        "\n",
        "# Tokenize the prompts in the dataset.\n",
        "# This example assumes that the model requires only 'input_ids'.\n",
        "# Adjust as necessary for your model, e.g., adding 'attention_mask'.\n",
        "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "\n",
        "# Split the dataset into training and testing sets (90% training, 10% testing)\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "train_data = train_test_split['train']\n",
        "test_data = train_test_split['test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del df\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "v5Cs525d8CgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmwdXOBGoZF7"
      },
      "source": [
        "We'll need to tokenize our data so the model can understand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H_wXxkhC8Yv"
      },
      "source": [
        "Split dataset into 90% for training and 10% for testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaBpEdgxwMds"
      },
      "source": [
        "### After Formatting, We should get something like this\n",
        "\n",
        "```json\n",
        "{\n",
        "\"text\":\"<s>[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]\n",
        "# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum</s>\",\n",
        "\"instruction\":\"Create a function to calculate the sum of a sequence of integers\",\n",
        "\"input\":\"[1, 2, 3, 4, 5]\",\n",
        "\"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in,\n",
        " sequence: sum += num return sum\"\n",
        "\"prompt\":\"<s>[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]\n",
        "# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum</s>\"\n",
        "\n",
        "}\n",
        "```\n",
        "\n",
        "While using SFT (**[Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/main/en/sft_trainer)**) for fine-tuning, we will be only passing in the “text” column of the dataset for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgAyy_xDamxg"
      },
      "outputs": [],
      "source": [
        "print(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzNQf6lkqo-T"
      },
      "source": [
        "## Step 4 - Apply Lora  \n",
        "Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMELsVV6q2my"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm3nXV988zew"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ4oR_hH9nF5"
      },
      "source": [
        "Use the following function to find out the linear layers for fine tuning.\n",
        "QLoRA paper : \"We find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total and that LoRA on all linear transformer block layers is required to match full finetuning performance.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acCr5AZ0831z"
      },
      "outputs": [],
      "source": [
        "def find_all_linear_names(model):\n",
        "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "  lora_module_names = set()\n",
        "  for name, module in model.named_modules():\n",
        "    if isinstance(module, cls):\n",
        "      names = name.split('.')\n",
        "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
        "      lora_module_names.remove('lm_head')\n",
        "  return list(lora_module_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhtO5dMr9Gq3"
      },
      "outputs": [],
      "source": [
        "modules = find_all_linear_names(model)\n",
        "print(modules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glEtbT3z_hme"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=modules,\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM'\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIWgYbz9C2ee"
      },
      "outputs": [],
      "source": [
        "#trainable, total = model.get_nb_trainable_parameters()\n",
        "#print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jGWwA25r-x0"
      },
      "source": [
        "## Step 5 - Run the training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlBiY1OzFZnN"
      },
      "source": [
        "Setting the training arguments:\n",
        "* for the reason of demo, we just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJxy4y9Owe4z"
      },
      "source": [
        "### Fine-Tuning with qLora and Supervised Fine-Tuning\n",
        "\n",
        "We're ready to fine-tune our model using qLora. For this tutorial, we'll use the `SFTTrainer` from the `trl` library for supervised fine-tuning. Ensure that you've installed the `trl` library as mentioned in the prerequisites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC4hx6IG3Jye"
      },
      "outputs": [],
      "source": [
        "model = model.to(device='cuda', dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-BCLCGS1NQk"
      },
      "outputs": [],
      "source": [
        "print(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQyMqLg5izHF"
      },
      "outputs": [],
      "source": [
        "#new code using SFTTrainer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=32, # ToDo: Try to fix memory issues and run with 16\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=0.03,\n",
        "    max_steps=100, # ToDo: Run for the full dataset\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=1,\n",
        "    optim='paged_adamw_8bit',\n",
        "    save_strategy='steps',\n",
        "    save_steps=5,\n",
        "    output_dir=CHECKPOINT_PATH,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    dataset_text_field='prompt',\n",
        "    peft_config=lora_config,\n",
        "    args=training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXtaw9qFcz6"
      },
      "source": [
        "Start the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXdeRUxUwhHk"
      },
      "source": [
        "### Let's start the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKzE8bDHWsgJ"
      },
      "outputs": [],
      "source": [
        "def find_latest_checkpoint(checkpoint_path):\n",
        "    \"\"\"\n",
        "    Finds the latest (highest-numbered) checkpoint directory.\n",
        "\n",
        "    Parameters:\n",
        "    checkpoint_path (str): The base path where checkpoint directories are stored.\n",
        "\n",
        "    Returns:\n",
        "    str: The path to the latest checkpoint directory, or None if no checkpoint found.\n",
        "    \"\"\"\n",
        "    # Regex to match checkpoint directories like 'checkpoint-500'\n",
        "    checkpoint_pattern = re.compile(r\"^checkpoint-\\d+$\")\n",
        "\n",
        "    # Get all directories in the checkpoint path\n",
        "    all_files = os.listdir(checkpoint_path)\n",
        "\n",
        "    # Filter out checkpoint directories\n",
        "    checkpoint_dirs = [f for f in all_files if checkpoint_pattern.match(f)]\n",
        "\n",
        "    # No checkpoint directories found\n",
        "    if not checkpoint_dirs:\n",
        "        return None\n",
        "\n",
        "    # Find the checkpoint directory with the highest step number\n",
        "    latest_checkpoint_dir = max(checkpoint_dirs, key=lambda x: int(x.split('-')[-1]))\n",
        "\n",
        "    # Return the full path to the latest checkpoint directory\n",
        "    return os.path.join(checkpoint_path, latest_checkpoint_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4HNvrh5FYqM"
      },
      "outputs": [],
      "source": [
        "# Suppress all warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "\n",
        "# Find the latest checkpoint\n",
        "latest_checkpoint_path = find_latest_checkpoint(CHECKPOINT_PATH)\n",
        "\n",
        "if latest_checkpoint_path and os.path.isdir(latest_checkpoint_path):\n",
        "    print(f\"Resuming training from the latest checkpoint: {latest_checkpoint_path}\")\n",
        "    trainer.train(resume_from_checkpoint=latest_checkpoint_path)\n",
        "else:\n",
        "    print(f\"No checkpoints found in: {CHECKPOINT_PATH}. Training will start from scratch.\")\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-TNSuMzCuSc"
      },
      "source": [
        "## Step 6 - Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HysfvJRzCStZ"
      },
      "outputs": [],
      "source": [
        "# Save the model to the repo\n",
        "model.save_pretrained(MODEL_PATH)\n",
        "tokenizer.save_pretrained(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbQ9N5Pcgeyh"
      },
      "outputs": [],
      "source": [
        "# # Or load from the file\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit= True,\n",
        "#     bnb_4bit_quant_type= 'nf4',\n",
        "#     bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "#     bnb_4bit_use_double_quant= False,\n",
        "#  )\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, quantization_config=bnb_config, device_map={\"\":0})\n",
        "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, add_eos_token=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhPnf0gIjbmN"
      },
      "source": [
        "### Save to Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkEXe_v-jh0l"
      },
      "source": [
        "### Save to Kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmaNNazandWS"
      },
      "outputs": [],
      "source": [
        "# Ensure the directory exists\n",
        "os.makedirs(KAGGLE_DATASET_PATH, exist_ok=True)\n",
        "\n",
        "# Move the model files to the directory\n",
        "!cp \"$MODEL_PATH\"/* \"$KAGGLE_DATASET_PATH\"/\n",
        "!ls $KAGGLE_DATASET_PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3oiSpOqrRL1"
      },
      "outputs": [],
      "source": [
        "# Add a metadata file\n",
        "metadata = {\n",
        "    'title': KAGGLE_DATASET_NAME,\n",
        "    'id': f\"{KAGGLE_USERNAME}/{KAGGLE_DATASET_NAME}\",\n",
        "    'licenses': [{'name': 'CC0-1.0'}], # ToDo: What license?\n",
        "    'description': 'A Mistral 7B model finetuned to recover Gemma prompts based on the output. This model is trained on Google Colab and is used for our submission to https://www.kaggle.com/competitions/llm-prompt-recovery.',\n",
        "    'subtitle': 'Finetuned Mistral 7B Model',\n",
        "    'keywords': ['LLM', 'Prompt', 'Recovery', 'Mistral', 'Gemma'],\n",
        "    'collaborators': ['Oliver Savolainen', 'Mark Bodracska', 'Valentina Lilova'],\n",
        "    'resources': [\n",
        "        # {\n",
        "        #     'path': 'file1.csv',\n",
        "        #     'description': 'Description of file1.csv contents.'\n",
        "        # },\n",
        "        # {\n",
        "        #     'path': 'file2.csv',\n",
        "        #     'description': 'Description of file2.csv contents.'\n",
        "        # }\n",
        "    ],\n",
        "}\n",
        "\n",
        "with open(os.path.join(KAGGLE_DATASET_PATH, 'dataset-metadata.json'), 'w') as f:\n",
        "    json.dump(metadata, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO3WDDKtbZ28"
      },
      "outputs": [],
      "source": [
        "# Add the test CSV file\n",
        "# ToDo: Add the real test CSV or remove\n",
        "!cp \"$TEST_FILE_PATH\" \"$KAGGLE_DATASET_PATH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcsKGeTusv5S"
      },
      "outputs": [],
      "source": [
        "# Upload the dataset to Kaggle\n",
        "# Create the first time\n",
        "!kaggle datasets create -p {KAGGLE_DATASET_PATH} --dir-mode zip\n",
        "# Update an existing dataset\n",
        "#!kaggle datasets version -p {KAGGLE_DATASET_PATH} -m \"Update dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki-m4hzHFqTu"
      },
      "source": [
        "## Step 7 - Evaluating the model qualitatively: run an inference!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32dee9bf"
      },
      "outputs": [],
      "source": [
        "# import re\n",
        "# def remove_special_characters(text):\n",
        "#     # This regex will match any character that is not a letter, number, or whitespace\n",
        "#     pattern = r'[^a-zA-Z0-9\\s]'\n",
        "#     text =text.replace(\"Transform\" ,\"improve\")\n",
        "#     text =text.replace(\"Reimagine\" ,\"rewrite\")\n",
        "#     # Replace these characters with an empty string\n",
        "#     clean_text = re.sub(pattern, '', text)\n",
        "#     return clean_text\n",
        "\n",
        "# def get_completion(prompt) -> str:\n",
        "#     device = \"cuda:0\"\n",
        "#     print('prompt')\n",
        "#     print(prompt)\n",
        "#     encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "#     model_inputs = encodeds.to(device)\n",
        "\n",
        "\n",
        "#     generated_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "#     generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "#     print('gen')\n",
        "#     print(generated_text)\n",
        "#     prompt_length = len(tokenizer.encode(prompt))\n",
        "#     print('output')\n",
        "#     prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
        "#     generated_tokens = generated_ids[0][len(prompt_tokens):]\n",
        "#     generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "#     print(generated_text)\n",
        "#     return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djDRGJlAAUvd"
      },
      "outputs": [],
      "source": [
        "# test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
        "# test_df['prompt'] = test_df.apply(lambda row: create_prompt(row['original_text'], row['rewritten_text']), axis=1)\n",
        "# test_df['prompt'] = test_df.apply(generate_prompt, axis=1)\n",
        "# test_df['rewrite_prompt'] = test_df.apply(lambda row: get_completion(row['prompt']), axis=1)\n",
        "# test_df = test_df[['id', 'rewrite_prompt']]\n",
        "# test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c62b9142"
      },
      "outputs": [],
      "source": [
        "# test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
        "# adject =[]\n",
        "# for index, row in test_df.iterrows():\n",
        "#         result = generate_prompt(create_prompt(row['original_text'], row['rewritten_text']))\n",
        "#         result  = remove_special_characters(result)\n",
        "#         print(result)\n",
        "#         test_df.at[index, 'rewrite_prompt'] =  result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# test_df = test_df[['id', 'rewrite_prompt']]\n",
        "# test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bfcdedb"
      },
      "outputs": [],
      "source": [
        "# test_df.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I40p19pyi3-I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}